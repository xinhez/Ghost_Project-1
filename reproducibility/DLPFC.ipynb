{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elk-mSRoB2bv"
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5deNrTo3UPZn"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "data_technique = 'dlpfc'\n",
        "\n",
        "groups = [\n",
        "        ['151507', '151508', '151509', '151510'],\n",
        "        ['151669', '151670', '151671', '151672'],\n",
        "        ['151673', '151674', '151675', '151676']\n",
        "]\n",
        "\n",
        "donor = {\n",
        "    '151507':'donor0', \n",
        "    '151508':'donor0', \n",
        "    '151509':'donor0', \n",
        "    '151510':'donor0',\n",
        "    '151669':'donor1',\n",
        "    '151670':'donor1',\n",
        "    '151671':'donor1',\n",
        "    '151672':'donor1',\n",
        "    '151673':'donor2',\n",
        "    '151674':'donor2',\n",
        "    '151675':'donor2',\n",
        "    '151676':'donor2'\n",
        "}\n",
        "\n",
        "all_batches = list(itertools.chain(*groups))\n",
        "test_batches = ['151507']\n",
        "\n",
        "\n",
        "device= 'cuda:0'\n",
        "\n",
        "\n",
        "data_path = 'data/DLPFC'\n",
        "batch_key = 'batch'\n",
        "modified_batch_key = 'batch_modified'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LLRemV7UUvj"
      },
      "source": [
        "#### Parameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wIoaEk_yDIs6"
      },
      "outputs": [],
      "source": [
        "train_epoch = 30\n",
        "train_batch = 512\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "use_batch_key = modified_batch_key\n",
        "\n",
        "clean_previous_evaluation_results = True\n",
        "clean_previous_inference_results = True\n",
        "checkpoint = 1\n",
        "\n",
        "verbose_train = True\n",
        "verbose_evaluation = True\n",
        "verbose_inference = True\n",
        "\n",
        "train_model = True\n",
        "evaluate_model = True\n",
        "infer_model = True\n",
        "\n",
        "root_path = f\"saved_results/{train_epoch}_epochs\"\n",
        "final_step = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmzEnKsG1hQu"
      },
      "source": [
        "#### Auto Programs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I83RcMOb1hQv"
      },
      "source": [
        "##### Generate Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mPR8FvxG1hQw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "label_key = 'cell_type'\n",
        "BATCH_TRAIN = 'train'\n",
        "BATCH_TEST = 'test'\n",
        "\n",
        "root_log_path = f'{root_path}'\n",
        "root_model_path = f'{root_path}/models'\n",
        "root_tensorboard_path = f'{root_path}/tensorboards'\n",
        "os.makedirs(root_log_path, exist_ok=True)\n",
        "os.makedirs(root_model_path, exist_ok=True)\n",
        "os.makedirs(root_tensorboard_path, exist_ok=True)\n",
        "\n",
        "def get_train_paths(test_batch):\n",
        "    return f'{root_log_path}/{test_batch}', f'{root_model_path}/{test_batch}', f'{root_tensorboard_path}/{test_batch}'\n",
        "\n",
        "def print_time(name, start_time):\n",
        "    print(f\"{name}: {((time.time()-start_time)/60):0.2f} mins\")\n",
        "\n",
        "def clean_results(log_path, model_path, tensorboard_path):\n",
        "    if log_path is not None and os.path.exists(log_path):\n",
        "        os.remove(log_path)\n",
        "    if model_path is not None and os.path.exists(model_path):\n",
        "        shutil.rmtree(model_path)\n",
        "    if tensorboard_path is not None and os.path.exists(tensorboard_path):\n",
        "        shutil.rmtree(tensorboard_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK9SInRzMATN"
      },
      "source": [
        "##### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRGs5U9hDIs7",
        "outputId": "b3b6e90a-d60c-482f-a10f-df9e2171a5ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "  \n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\anndata\\_core\\anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"obs\")\n"
          ]
        }
      ],
      "source": [
        "import anndata as ad\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "\n",
        "def generate_adata(data, nonnan_indices, cell_type_label, cols, rows, batch):\n",
        "    data = data.loc[data.index[nonnan_indices]]\n",
        "    adata=ad.AnnData(X=np.array(data),obs=list(data.index))\n",
        "    adata.obs[label_key]  = cell_type_label\n",
        "    adata.obs['imagecol'] = cols \n",
        "    adata.obs['imagerow'] = rows\n",
        "    adata.obs[batch_key]  = batch\n",
        "    return adata\n",
        "\n",
        "dlpfc_adatas = dict()\n",
        "for batch in all_batches:\n",
        "    data_batch      = pd.read_csv(f\"{data_path}/{batch}/tissue_positions_list.csv\", header=None, index_col=0)\n",
        "    rna_data        = pd.read_csv(f'{data_path}/{batch}/DLPFC_spatial_simulation_mrna_marker.csv',index_col=0)\n",
        "    morph_data      = pd.read_csv(f'{data_path}/{batch}/DLPFC_spatial_simulation_morph.csv',index_col=0).T\n",
        "    mrna_niche_data = pd.read_csv(f'{data_path}/{batch}/DLPFC_spatial_simulation_niche_mrna_mp.csv',index_col=0).T\n",
        "    cell_type_label = np.array(pd.read_csv(f\"{data_path}/{batch}/cluster_labels_{batch}.csv\",index_col=0).astype(str)['ground_truth'])\n",
        "\n",
        "    nonnan_indices = np.where(cell_type_label != 'nan')[0].astype(int)\n",
        "\n",
        "    rows = np.array(data_batch.loc[list(rna_data.index),4])\n",
        "    cols = np.array(data_batch.loc[list(rna_data.index),5])\n",
        "\n",
        "    cell_type_label = cell_type_label[nonnan_indices]\n",
        "    rows = rows[nonnan_indices]\n",
        "    cols = cols[nonnan_indices]\n",
        "\n",
        "    adata_rna = generate_adata(rna_data, nonnan_indices, cell_type_label, cols, rows, batch)\n",
        "    adata_morph = generate_adata(morph_data, nonnan_indices, cell_type_label, cols, rows, batch)\n",
        "    adata_mrna_niche = generate_adata(mrna_niche_data, nonnan_indices, cell_type_label, cols, rows, batch)\n",
        "\n",
        "    dlpfc_adatas[batch] = [adata_rna, adata_morph, adata_mrna_niche]\n",
        "    \n",
        "adata_rna_all = ad.concat([dlpfc_adatas[batch][0] for batch in all_batches])\n",
        "adata_morph_all = ad.concat([dlpfc_adatas[batch][1] for batch in all_batches])\n",
        "adata_mrna_niche_all = ad.concat([dlpfc_adatas[batch][2] for batch in all_batches])\n",
        "\n",
        "sc.pp.normalize_total(adata_rna_all)\n",
        "sc.pp.log1p(adata_rna_all)\n",
        "sc.pp.scale(adata_rna_all, max_value=4)\n",
        "\n",
        "def split_data(test_batch):\n",
        "    adata_rna_train  = adata_rna_all[adata_rna_all.obs[batch_key] != test_batch]\n",
        "    adata_morph_train  = adata_morph_all[adata_morph_all.obs[batch_key] != test_batch]\n",
        "    adata_mrna_niche_train  = adata_mrna_niche_all[adata_mrna_niche_all.obs[batch_key] != test_batch]\n",
        "\n",
        "    adata_rna_test  = adata_rna_all[adata_rna_all.obs[batch_key] == test_batch]\n",
        "    adata_morph_test  = adata_morph_all[adata_morph_all.obs[batch_key] == test_batch]\n",
        "    adata_mrna_niche_test  = adata_mrna_niche_all[adata_mrna_niche_all.obs[batch_key] == test_batch]\n",
        "\n",
        "    adata_rna_train.obs[modified_batch_key] = BATCH_TRAIN\n",
        "    adata_morph_train.obs[modified_batch_key]  = BATCH_TRAIN\n",
        "    adata_mrna_niche_train.obs[modified_batch_key] = BATCH_TRAIN\n",
        "\n",
        "    adata_rna_test.obs[modified_batch_key] = BATCH_TEST\n",
        "    adata_morph_test.obs[modified_batch_key]  = BATCH_TEST\n",
        "    adata_mrna_niche_test.obs[modified_batch_key] = BATCH_TEST\n",
        "    \n",
        "    return [adata_rna_train, adata_morph_train, adata_mrna_niche_train], [adata_rna_test, adata_morph_test, adata_mrna_niche_test]\n",
        "\n",
        "def concat_adatas(adatas_train, adatas_test):\n",
        "    return [ad.concat([adata_train,adata_test]) for adata_train, adata_test in zip(adatas_train, adatas_test)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXARR_o1MATP"
      },
      "source": [
        "#### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VLrTo7vDIs8",
        "outputId": "7d5f1bda-f21c-48b7-c1a5-fecaa6dedce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================== 151507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:55: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:56: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:57: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:59: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:60: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:61: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data: 0.01 mins\n",
            "Register data: 0.12 mins\n",
            "\n",
            "TRAIN\n",
            "\n",
            "    (Epoch 1 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.871459484100342\n",
            "        discriminator: 0.48772650957107544\n",
            "        generator: 0.013832403346896172\n",
            "        reconstruction: 5.88614559173584\n",
            "        translation: 6.483118534088135\n",
            "    Losses\n",
            "        contrastive: 5.183070182800293\n",
            "        discriminator: 0.49928200244903564\n",
            "        generator: 0.0006626107497140765\n",
            "        reconstruction: 4.028621196746826\n",
            "        translation: 5.511995792388916\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_1.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 3.6536924839019775\n",
            "    Losses\n",
            "        cross_entropy: 0.38836532831192017\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_1.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.3417779505252838\n",
            "    Losses\n",
            "        cross_entropy: 0.2563866078853607\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_1.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.21524259448051453\n",
            "    Losses\n",
            "        cross_entropy: 0.19058139622211456\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_1.pt\n",
            "\n",
            "    (Epoch 2 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 4.826323986053467\n",
            "        discriminator: 0.49685102701187134\n",
            "        generator: 0.0029088761657476425\n",
            "        reconstruction: 4.052550315856934\n",
            "        translation: 5.706382751464844\n",
            "    Losses\n",
            "        contrastive: 4.370903491973877\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.8483946323394775\n",
            "        translation: 5.530341625213623\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_2.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 7.416754722595215\n",
            "    Losses\n",
            "        cross_entropy: 3.73331356048584\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_2.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.6890924572944641\n",
            "    Losses\n",
            "        cross_entropy: 0.2814094126224518\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_2.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.27338412404060364\n",
            "    Losses\n",
            "        cross_entropy: 0.19753602147102356\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_2.pt\n",
            "\n",
            "    (Epoch 3 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 4.899588108062744\n",
            "        discriminator: 0.4999557137489319\n",
            "        generator: 4.062249354319647e-05\n",
            "        reconstruction: 4.342684268951416\n",
            "        translation: 5.778565883636475\n",
            "    Losses\n",
            "        contrastive: 3.1848702430725098\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 4.060885429382324\n",
            "        translation: 5.672986030578613\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_3.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 14.045690536499023\n",
            "    Losses\n",
            "        cross_entropy: 1.2996174097061157\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_3.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.5541979074478149\n",
            "    Losses\n",
            "        cross_entropy: 0.2380327731370926\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_3.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.31136268377304077\n",
            "    Losses\n",
            "        cross_entropy: 0.1673981100320816\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_3.pt\n",
            "\n",
            "    (Epoch 4 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 4.968106746673584\n",
            "        discriminator: 0.4974517524242401\n",
            "        generator: 0.0023537371307611465\n",
            "        reconstruction: 4.344964027404785\n",
            "        translation: 5.784745216369629\n",
            "    Losses\n",
            "        contrastive: 4.113490104675293\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.8622488975524902\n",
            "        translation: 5.673459053039551\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_4.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 62.3300895690918\n",
            "    Losses\n",
            "        cross_entropy: 1.5629668235778809\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_4.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 1.2208021879196167\n",
            "    Losses\n",
            "        cross_entropy: 1.187248706817627\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_4.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.7886887192726135\n",
            "    Losses\n",
            "        cross_entropy: 0.6066573262214661\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_4.pt\n",
            "\n",
            "    (Epoch 5 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 7.621359825134277\n",
            "        discriminator: 0.49955055117607117\n",
            "        generator: 0.00041538215009495616\n",
            "        reconstruction: 4.244369029998779\n",
            "        translation: 5.792138576507568\n",
            "    Losses\n",
            "        contrastive: 5.695044040679932\n",
            "        discriminator: 0.49952733516693115\n",
            "        generator: 0.0004368211084511131\n",
            "        reconstruction: 3.8793933391571045\n",
            "        translation: 5.721137523651123\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_5.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 82.40129089355469\n",
            "    Losses\n",
            "        cross_entropy: 4.029313087463379\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_5.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 2.04962158203125\n",
            "    Losses\n",
            "        cross_entropy: 0.7859624624252319\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_5.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 1.429630994796753\n",
            "    Losses\n",
            "        cross_entropy: 1.3808413743972778\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_5.pt\n",
            "\n",
            "    (Epoch 6 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 6.555854320526123\n",
            "        discriminator: 0.49982312321662903\n",
            "        generator: 0.00016347304335795343\n",
            "        reconstruction: 4.145769119262695\n",
            "        translation: 5.763538360595703\n",
            "    Losses\n",
            "        contrastive: 5.668464660644531\n",
            "        discriminator: 0.49989742040634155\n",
            "        generator: 9.443006274523214e-05\n",
            "        reconstruction: 3.847774028778076\n",
            "        translation: 5.656926155090332\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_6.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 12.14586067199707\n",
            "    Losses\n",
            "        cross_entropy: 0.8034430742263794\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_6.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.6318240761756897\n",
            "    Losses\n",
            "        cross_entropy: 0.4225822389125824\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_6.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.7002803683280945\n",
            "    Losses\n",
            "        cross_entropy: 0.3327500820159912\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_6.pt\n",
            "\n",
            "    (Epoch 7 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.597903728485107\n",
            "        discriminator: 0.5\n",
            "        generator: 1.1983260737868928e-11\n",
            "        reconstruction: 4.670562267303467\n",
            "        translation: 5.73710298538208\n",
            "    Losses\n",
            "        contrastive: 5.192845344543457\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 4.20568323135376\n",
            "        translation: 5.682379245758057\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_7.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 42.31119918823242\n",
            "    Losses\n",
            "        cross_entropy: 1.8361382484436035\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_7.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 1.5677411556243896\n",
            "    Losses\n",
            "        cross_entropy: 0.4930976331233978\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_7.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.522419273853302\n",
            "    Losses\n",
            "        cross_entropy: 0.2579934895038605\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_7.pt\n",
            "\n",
            "    (Epoch 8 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.2617316246032715\n",
            "        discriminator: 0.4999941885471344\n",
            "        generator: 5.360370323614916e-06\n",
            "        reconstruction: 3.974883794784546\n",
            "        translation: 5.732072353363037\n",
            "    Losses\n",
            "        contrastive: 4.47742223739624\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.818389892578125\n",
            "        translation: 5.6627583503723145\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_8.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 5.71211051940918\n",
            "    Losses\n",
            "        cross_entropy: 0.6043103337287903\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_8.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.4092555344104767\n",
            "    Losses\n",
            "        cross_entropy: 0.5503133535385132\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_8.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.2932609021663666\n",
            "    Losses\n",
            "        cross_entropy: 0.17691867053508759\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_8.pt\n",
            "\n",
            "    (Epoch 9 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.746429443359375\n",
            "        discriminator: 0.4964390695095062\n",
            "        generator: 0.0032901435624808073\n",
            "        reconstruction: 3.8285014629364014\n",
            "        translation: 5.669423580169678\n",
            "    Losses\n",
            "        contrastive: 5.547414779663086\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.7410366535186768\n",
            "        translation: 5.615156650543213\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_9.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 1.7883741855621338\n",
            "    Losses\n",
            "        cross_entropy: 0.2795967757701874\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_9.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.34390461444854736\n",
            "    Losses\n",
            "        cross_entropy: 0.20166276395320892\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_9.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.16402587294578552\n",
            "    Losses\n",
            "        cross_entropy: 0.12430237233638763\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_9.pt\n",
            "\n",
            "    (Epoch 10 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.597589015960693\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.7314651012420654\n",
            "        translation: 5.611684799194336\n",
            "    Losses\n",
            "        contrastive: 5.485493183135986\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.6271653175354004\n",
            "        translation: 5.550882816314697\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_10.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.5360171794891357\n",
            "    Losses\n",
            "        cross_entropy: 0.11921268701553345\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_10.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.14596478641033173\n",
            "    Losses\n",
            "        cross_entropy: 0.09604205936193466\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_10.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.11466647684574127\n",
            "    Losses\n",
            "        cross_entropy: 0.09174869954586029\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_10.pt\n",
            "\n",
            "    (Epoch 11 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.533590316772461\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.5271570682525635\n",
            "        translation: 5.527064323425293\n",
            "    Losses\n",
            "        contrastive: 5.457045078277588\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.482100486755371\n",
            "        translation: 5.452705383300781\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_11.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.22380833327770233\n",
            "    Losses\n",
            "        cross_entropy: 0.12783367931842804\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_11.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.1314830631017685\n",
            "    Losses\n",
            "        cross_entropy: 0.06392716616392136\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_11.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.12869828939437866\n",
            "    Losses\n",
            "        cross_entropy: 0.06186889111995697\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_11.pt\n",
            "\n",
            "    (Epoch 12 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.46362829208374\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.4518096446990967\n",
            "        translation: 5.443304061889648\n",
            "    Losses\n",
            "        contrastive: 5.4042863845825195\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.4314541816711426\n",
            "        translation: 5.401421070098877\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_12.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.21103960275650024\n",
            "    Losses\n",
            "        cross_entropy: 0.0913146436214447\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_12.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.16780860722064972\n",
            "    Losses\n",
            "        cross_entropy: 0.06064043566584587\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_12.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.0740799829363823\n",
            "    Losses\n",
            "        cross_entropy: 0.05585542321205139\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_12.pt\n",
            "\n",
            "    (Epoch 13 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.397632122039795\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.4362754821777344\n",
            "        translation: 5.391357421875\n",
            "    Losses\n",
            "        contrastive: 5.330600738525391\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.408517599105835\n",
            "        translation: 5.318787574768066\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_13.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.1602967530488968\n",
            "    Losses\n",
            "        cross_entropy: 0.17341117560863495\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_13.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.10953894257545471\n",
            "    Losses\n",
            "        cross_entropy: 0.05873585864901543\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_13.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.07161608338356018\n",
            "    Losses\n",
            "        cross_entropy: 0.04068536311388016\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_13.pt\n",
            "\n",
            "    (Epoch 14 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.329835891723633\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.4116694927215576\n",
            "        translation: 5.328140735626221\n",
            "    Losses\n",
            "        contrastive: 5.259794235229492\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.383739948272705\n",
            "        translation: 5.276005268096924\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_14.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.12202583998441696\n",
            "    Losses\n",
            "        cross_entropy: 0.04237154498696327\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_14.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.04953162372112274\n",
            "    Losses\n",
            "        cross_entropy: 0.03921870142221451\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_14.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.04894278571009636\n",
            "    Losses\n",
            "        cross_entropy: 0.02294679544866085\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_14.pt\n",
            "\n",
            "    (Epoch 15 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.260171890258789\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.3911237716674805\n",
            "        translation: 5.26529598236084\n",
            "    Losses\n",
            "        contrastive: 5.1818084716796875\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.3742902278900146\n",
            "        translation: 5.187165260314941\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_15.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.1122267097234726\n",
            "    Losses\n",
            "        cross_entropy: 0.0961916521191597\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_15.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.26265114545822144\n",
            "    Losses\n",
            "        cross_entropy: 0.051896702498197556\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_15.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.09242049604654312\n",
            "    Losses\n",
            "        cross_entropy: 0.04396279528737068\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_15.pt\n",
            "\n",
            "    (Epoch 16 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.2029218673706055\n",
            "        discriminator: 0.499997079372406\n",
            "        generator: 2.67988616542425e-06\n",
            "        reconstruction: 3.397434949874878\n",
            "        translation: 5.228442668914795\n",
            "    Losses\n",
            "        contrastive: 5.05003023147583\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.3222930431365967\n",
            "        translation: 5.175029277801514\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_16.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.2817901372909546\n",
            "    Losses\n",
            "        cross_entropy: 0.07533218711614609\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_16.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.14228013157844543\n",
            "    Losses\n",
            "        cross_entropy: 0.0594487302005291\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_16.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.06770342588424683\n",
            "    Losses\n",
            "        cross_entropy: 0.047281283885240555\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_16.pt\n",
            "\n",
            "    (Epoch 17 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.154240608215332\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.3049676418304443\n",
            "        translation: 5.175617218017578\n",
            "    Losses\n",
            "        contrastive: 5.015810489654541\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.237760066986084\n",
            "        translation: 5.104561805725098\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_17.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.20643946528434753\n",
            "    Losses\n",
            "        cross_entropy: 0.03756750747561455\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_17.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.0358453132212162\n",
            "    Losses\n",
            "        cross_entropy: 0.02444341965019703\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_17.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.038971368223428726\n",
            "    Losses\n",
            "        cross_entropy: 0.030998578295111656\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_17.pt\n",
            "\n",
            "    (Epoch 18 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 5.026120185852051\n",
            "        discriminator: 0.499997079372406\n",
            "        generator: 2.67988616542425e-06\n",
            "        reconstruction: 3.2461585998535156\n",
            "        translation: 5.109999179840088\n",
            "    Losses\n",
            "        contrastive: 4.894608974456787\n",
            "        discriminator: 0.4999825954437256\n",
            "        generator: 1.60793169925455e-05\n",
            "        reconstruction: 3.2097909450531006\n",
            "        translation: 5.032707691192627\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_18.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.1275789588689804\n",
            "    Losses\n",
            "        cross_entropy: 0.06212382763624191\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_18.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.048316311091184616\n",
            "    Losses\n",
            "        cross_entropy: 0.02931816130876541\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_18.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.022224143147468567\n",
            "    Losses\n",
            "        cross_entropy: 0.029531141743063927\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_18.pt\n",
            "\n",
            "    (Epoch 19 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 4.892857551574707\n",
            "        discriminator: 0.4999884068965912\n",
            "        generator: 1.0719544661697e-05\n",
            "        reconstruction: 3.199897527694702\n",
            "        translation: 5.044013977050781\n",
            "    Losses\n",
            "        contrastive: 4.770240306854248\n",
            "        discriminator: 0.4999941885471344\n",
            "        generator: 5.3597723308485e-06\n",
            "        reconstruction: 3.169682025909424\n",
            "        translation: 4.97224760055542\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_19.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.1088457703590393\n",
            "    Losses\n",
            "        cross_entropy: 0.04439510032534599\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_19.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.038681935518980026\n",
            "    Losses\n",
            "        cross_entropy: 0.01980150304734707\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_19.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.02963726595044136\n",
            "    Losses\n",
            "        cross_entropy: 0.021883711218833923\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_19.pt\n",
            "\n",
            "    (Epoch 20 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 4.819199562072754\n",
            "        discriminator: 0.49994462728500366\n",
            "        generator: 5.1056787924608216e-05\n",
            "        reconstruction: 3.1967499256134033\n",
            "        translation: 5.004755973815918\n",
            "    Losses\n",
            "        contrastive: 4.690533638000488\n",
            "        discriminator: 0.5\n",
            "        generator: 0.0\n",
            "        reconstruction: 3.1874706745147705\n",
            "        translation: 4.94636344909668\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_0_translation/epoch_20.pt\n",
            "    ========== Schedule 1: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.11124783009290695\n",
            "    Losses\n",
            "        cross_entropy: 0.07697934657335281\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_1_classification/epoch_20.pt\n",
            "    ========== Schedule 2: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.04596695676445961\n",
            "    Losses\n",
            "        cross_entropy: 0.00988656748086214\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_2_classification/epoch_20.pt\n",
            "    ========== Schedule 3: classification ==========\n",
            "    Losses\n",
            "        cross_entropy: 0.028047213330864906\n",
            "    Losses\n",
            "        cross_entropy: 0.014710028655827045\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/best.pt\n",
            "            Saving model to saved_results/50_epochs/models/151507/train_3_classification/epoch_20.pt\n",
            "\n",
            "    (Epoch 21 / 50)\n",
            "    ========== Schedule 0: translation ==========\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30384\\3062517548.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m             model.train(\n\u001b[0;32m     38\u001b[0m                 \u001b[1;34m'supervised_group_identification'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0msave_best_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             )\n\u001b[0;32m     41\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madatas_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_index_evaluate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_key_evaluate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\xinhez\\Research\\UnitedNet\\src\\interface.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, task_or_schedules, adatas_validate, label_index_validate, label_key_validate, n_epoch, learning_rate, batch_size, save_best_model, checkpoint)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         )\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\xinhez\\Research\\UnitedNet\\src\\managers\\task.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, schedules, model, data, data_validate, batch_size, n_epoch, learning_rate, logger, model_path, save_best_model, checkpoint, writer, random_seed)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mrandom_seed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         )\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\xinhez\\Research\\UnitedNet\\src\\managers\\task.py\u001b[0m in \u001b[0;36mtrain_with_schedules\u001b[1;34m(self, logger, model, n_epoch, data, data_validate, batch_size, save_best_model, checkpoint, writer, random_seed)\u001b[0m\n\u001b[0;32m    191\u001b[0m                     \u001b[0mschedule\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[0mtrain_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                     \u001b[0mwriter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                 )\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\xinhez\\Research\\UnitedNet\\src\\managers\\task.py\u001b[0m in \u001b[0;36mrun_through_data\u001b[1;34m(self, logger, model, dataloader, epoch, schedule, train_model, infer_model, save_best_model, checkpoint_model_name, writer)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator_requested\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 )\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 losses = utils.amplify_value_dictionary_by_sample_size(\n\u001b[0;32m     66\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\xinhez\\Research\\UnitedNet\\src\\managers\\schedule.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, model, train_model)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\xinhez\\Research\\UnitedNet\\src\\managers\\loss.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_modality\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_inds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 )\n\u001b[1;32m--> 260\u001b[1;33m                 \u001b[0mneg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_inds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_inds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 labels = torch.zeros(\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from src import UnitedNet\n",
        "import anndata as ad\n",
        "import os, shutil\n",
        "import time \n",
        "\n",
        "def set_model_paths(model, log_path, model_path, tensorboard_path):\n",
        "    clean_results(log_path, model_path, tensorboard_path)\n",
        "    model.set_log_path(log_path)\n",
        "    model.set_model_path(model_path)\n",
        "    model.set_tensorboard_path(tensorboard_path)\n",
        "\n",
        "if train_model:\n",
        "    for test_batch in test_batches:\n",
        "        print('='*20, test_batch)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        adatas_train, adatas_test = split_data(test_batch)\n",
        "        print_time('Split data', start_time)\n",
        "\n",
        "        model = UnitedNet(device=device)\n",
        "\n",
        "        # ======================================== Register Data ========================================\n",
        "        checkpoint_time = time.time()\n",
        "        model.register_anndatas(\n",
        "            adatas_train, \n",
        "            label_index=0, label_key=label_key, \n",
        "            technique=data_technique,\n",
        "        )\n",
        "        print_time('Register data', checkpoint_time)\n",
        "\n",
        "        # ======================================== Transfer ========================================\n",
        "        if train_model:\n",
        "            checkpoint_time = time.time()\n",
        "            set_model_paths(model, *get_train_paths(test_batch))\n",
        "            model.set_verbose(verbose_train)\n",
        "            model.train(\n",
        "                'supervised_group_identification', n_epoch=train_epoch, learning_rate=learning_rate, batch_size=train_batch,\n",
        "                save_best_model=True, checkpoint=checkpoint,\n",
        "            )\n",
        "            model.evaluate(adatas_test, label_index_evaluate=0, label_key_evaluate=label_key)\n",
        "            model.transfer(\n",
        "                'supervised_group_identification', n_epoch=train_epoch, learning_rate=learning_rate, batch_size=train_batch,\n",
        "                adatas_transfer=adatas_test,\n",
        "                save_best_model=True, checkpoint=checkpoint,\n",
        "            )\n",
        "\n",
        "        print_time(f'Total for batch {test_batch}', start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRLdFX-T2MUJ"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "from src import UnitedNet\n",
        "def evaluate_adatas(path, adatas):\n",
        "    model = UnitedNet(\n",
        "        device=device,\n",
        "        log_path=None,\n",
        "        model_path=None,\n",
        "        tensorboard_path=None,\n",
        "        verbose=False,\n",
        "    )\n",
        "    model.load_model(path)\n",
        "\n",
        "    return model.evaluate(\n",
        "        adatas,\n",
        "        label_index_evaluate=0, label_key_evaluate=label_key,\n",
        "        batch_index_evaluate=0, batch_key_evaluate=use_batch_key,\n",
        "    )['ari']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzqUEUYK2J0v",
        "outputId": "2d3ff430-f898-4567-b4b1-ab056f4eeda0"
      },
      "outputs": [],
      "source": [
        "final_model_path = f'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\n",
        "aris = []\n",
        "for epoch in range(1, train_epoch+1):\n",
        "    adatas_train, adatas_test = split_data(test_batch)\n",
        "    adatas_all = concat_adatas(adatas_train, adatas_test)\n",
        "\n",
        "    model_path = f'{final_model_path}/epoch_{epoch}.pt'\n",
        "    aris.append(evaluate_adatas(model_path, adatas_test))\n",
        "\n",
        "for epoch in range(1, train_epoch+1):\n",
        "    print('epoch', epoch, aris[epoch-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGATLbJKMATU"
      },
      "source": [
        "#### Infer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_6pdX11hQ1"
      },
      "source": [
        "##### Infer Commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLL1F7b6MATU"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "from src import UnitedNet\n",
        "def infer_adatas(path, adatas, eval_only=True):\n",
        "    model = UnitedNet(\n",
        "        device=device,\n",
        "        log_path=None,\n",
        "        model_path=None,\n",
        "        tensorboard_path=None,\n",
        "        verbose=True,\n",
        "    )\n",
        "    model.load_model(path)\n",
        "\n",
        "    model.evaluate(\n",
        "        adatas,\n",
        "        label_index_evaluate=0, label_key_evaluate=label_key,\n",
        "        batch_index_evaluate=0, batch_key_evaluate=use_batch_key,\n",
        "    )\n",
        "\n",
        "    if not eval_only:\n",
        "      adata_inferred = model.infer(\n",
        "          adatas,\n",
        "          modalities_provided=list(range(len(adatas))),\n",
        "          batch_index_infer=0, batch_key_infer=use_batch_key, \n",
        "          modality_sizes=[adata.shape[1] for adata in adatas]\n",
        "      )\n",
        "      \n",
        "      adata_inferred.obs[modified_batch_key] = list(adatas[0].obs[modified_batch_key])\n",
        "      sc.pl.umap(adata_inferred, color=[modified_batch_key])\n",
        "\n",
        "      adata_inferred.obs['batch'] = list(adatas[0].obs['batch'])\n",
        "      sc.pl.umap(adata_inferred, color=['batch'])\n",
        "      \n",
        "      sc.pl.umap(adata_inferred, color=['predicted_label'])\n",
        "      \n",
        "      adata_inferred.obs[label_key] = list(adatas[0].obs[label_key])\n",
        "      sc.pl.umap(adata_inferred, color=[label_key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4596WEwMATW"
      },
      "source": [
        "##### Infer on train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9xkneiQrOQpV",
        "outputId": "77e6ab8a-4a1b-44d3-d1e0-2b3a264f7ad6"
      },
      "outputs": [],
      "source": [
        "final_model_path = f'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\n",
        "for epoch in range(1, train_epoch+1):\n",
        "    print('='*20, 'epoch', epoch)\n",
        "\n",
        "    adatas_train, adatas_test = split_data(test_batch)\n",
        "    adatas_all = concat_adatas(adatas_train, adatas_test)\n",
        "\n",
        "    model_path = f'{final_model_path}/epoch_{epoch}.pt'\n",
        "    infer_adatas(model_path, adatas_test)\n",
        "    infer_adatas(model_path, adatas_all, eval_only=False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P1xrExR9t-0m",
        "outputId": "8d4f74a2-55d3-403b-98b4-772e2b3053ad"
      },
      "outputs": [],
      "source": [
        "final_model_path = f'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\n",
        "for epoch in [train_epoch]:\n",
        "    print('='*20, 'epoch', epoch)\n",
        "\n",
        "    adatas_train, adatas_test = split_data(test_batch)\n",
        "    adatas_all = concat_adatas(adatas_train, adatas_test)\n",
        "\n",
        "    model_path = f'{final_model_path}/epoch_{epoch}.pt'\n",
        "    infer_adatas(model_path, adatas_test)\n",
        "    infer_adatas(model_path, adatas_all, eval_only=False)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xts0jTbV1hQs",
        "elk-mSRoB2bv",
        "I83RcMOb1hQv",
        "OK9SInRzMATN",
        "3m_6pdX11hQ1"
      ],
      "machine_shape": "hm",
      "name": "train_then_transfer C0_T1_C3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8dd4d9bf6ad6a171f666a3d65bcfced95842184592f01e2d9451b920ba48bd03"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('MMCGAN_ET')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
