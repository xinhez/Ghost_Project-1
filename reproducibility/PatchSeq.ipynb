{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "device= 'cuda:0'\n",
        "\n",
        "data_technique = 'patchseq'\n",
        "data_path = 'data/PatchSeq'\n",
        "experiment = 'single_layer'\n",
        "\n",
        "verbose = True\n",
        "\n",
        "train_epoch = 50\n",
        "learning_rate = 0.001\n",
        "train_batch = 16\n",
        "checkpoint = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Auto Programs and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "label_key = 'cell_type'\n",
        "root_path = f'saved_patchseq/{experiment}'\n",
        "os.makedirs(root_path, exist_ok=True)\n",
        "\n",
        "log_path = f'{root_path}/log'\n",
        "model_path = f'{root_path}/models'\n",
        "tensorboard_path = f'{root_path}/tensorboards'\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.makedirs(tensorboard_path, exist_ok=True)\n",
        "\n",
        "def print_time(name, start_time):\n",
        "    print(f\"{name}: {((time.time()-start_time)/60):0.2f} mins\")\n",
        "\n",
        "def clean_results(lp, mp, tp):\n",
        "    if lp is not None and os.path.exists(lp):\n",
        "        os.remove(lp)\n",
        "    if mp is not None and os.path.exists(mp):\n",
        "        shutil.rmtree(mp)\n",
        "    if tp is not None and os.path.exists(tp):\n",
        "        shutil.rmtree(tp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:62: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "C:\\ProgramData\\Anaconda3\\envs\\untest\\lib\\site-packages\\ipykernel_launcher.py:64: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n"
          ]
        }
      ],
      "source": [
        "from math import nan\n",
        "import scipy.io as sio\n",
        "import json\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "alen_label = pd.read_excel(f'{data_path}/MET_data.xlsx')\n",
        "\n",
        "Edat = 'E_pcipfx'\n",
        "dir_pth = data_path\n",
        "# Data operations:\n",
        "D = sio.loadmat(dir_pth + '/PS_v5_beta_0-4_pc_scaled_ipfx_eqTE.mat', squeeze_me=True)\n",
        "with open(dir_pth + '/E_names.json') as f:\n",
        "    ephys_names = json.load(f)\n",
        "D[Edat] = np.concatenate([D['E_pc_scaled'], D['E_feature']], axis=1)\n",
        "D['pcipfx_names'] = np.concatenate([D['pc_name'], D['feature_name']])\n",
        "temp = [ephys_names[f] for f in D['pcipfx_names']]\n",
        "D['pcipfx_names'] = np.array(temp)\n",
        "good_idx = ~np.isnan(D[Edat]).any(axis=1)\n",
        "E_dat = D[Edat][good_idx]\n",
        "T_dat = D['T_dat'][good_idx]\n",
        "\n",
        "id_label = D['E_spec_id_label'][good_idx]\n",
        "\n",
        "\n",
        "morph_feature = pd.read_csv(dir_pth + '/morph_features_gouwens.csv', index_col=0)\n",
        "M_dat = morph_feature.iloc[:,1:].values\n",
        "cell_id = morph_feature['cell_id']\n",
        "\n",
        "cell_id,x_ind,_ = np.intersect1d(cell_id, alen_label['Specimen ID'], return_indices=True)\n",
        "M_dat = M_dat[x_ind]\n",
        "\n",
        "xy, x_ind, y_ind = np.intersect1d(id_label, cell_id, return_indices=True)\n",
        "E_dat = E_dat[x_ind]\n",
        "T_dat = T_dat[x_ind]\n",
        "cell_type_MET = pd.Series(cell_id[y_ind]).map(dict(zip(alen_label['Specimen ID'],alen_label['MET type'])))\n",
        "cell_type_T = pd.Series(cell_id[y_ind]).map(dict(zip(D['T_spec_id_label'],D['cluster'])))\n",
        "cell_type = cell_type_MET\n",
        "M_dat = M_dat[y_ind]\n",
        "\n",
        "\n",
        "\n",
        "# cell_type_less = [ct.split('-')[0] for ct in cell_type.values]\n",
        "\n",
        "scaler_T = preprocessing.StandardScaler().fit(T_dat)\n",
        "T_dat = scaler_T.transform(T_dat)\n",
        "scaler_E = preprocessing.StandardScaler().fit(E_dat)\n",
        "E_dat = scaler_E.transform(E_dat)\n",
        "scaler_M = preprocessing.StandardScaler().fit(M_dat)\n",
        "M_dat = scaler_M.transform(M_dat)\n",
        "\n",
        "cell_type_T_less = np.array([_ls.split(' ')[0] for _ls in cell_type_T])\n",
        "\n",
        "E_dat = E_dat[cell_type_T_less!='Serpinf1',:]\n",
        "M_dat = M_dat[cell_type_T_less!='Serpinf1',:]\n",
        "\n",
        "cell_type_T_less = cell_type_T_less[cell_type_T_less!='Serpinf1']\n",
        "\n",
        "import anndata as ad\n",
        "adataE = ad.AnnData(E_dat)\n",
        "adataE.obs[label_key] = cell_type_T_less\n",
        "adataM = ad.AnnData(M_dat) \n",
        "adataM.obs[label_key] = cell_type_T_less\n",
        "adatas_train = [adataE, adataM]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        Vip\n",
              "1        Sst\n",
              "2        Sst\n",
              "3        Sst\n",
              "4        Sst\n",
              "       ...  \n",
              "442    Lamp5\n",
              "443    Lamp5\n",
              "444      Vip\n",
              "445      Vip\n",
              "446    Pvalb\n",
              "Name: cell_type, Length: 447, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adatas_train[0].obs[label_key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXARR_o1MATP"
      },
      "source": [
        "#### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VLrTo7vDIs8",
        "outputId": "7d5f1bda-f21c-48b7-c1a5-fecaa6dedce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Register data: 0.00 mins\n",
            "Model(\n",
            "  (modules_by_names): ModuleDict(\n",
            "    (encoders): ModuleList(\n",
            "      (0): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=1024, bias=True)\n",
            "          (1): ReLUActivation()\n",
            "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "          (3): ReLUActivation()\n",
            "          (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (5): ReLUActivation()\n",
            "          (6): Linear(in_features=256, out_features=128, bias=True)\n",
            "          (7): ReLUActivation()\n",
            "          (8): Linear(in_features=128, out_features=68, bias=True)\n",
            "          (9): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=514, out_features=1024, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "          (2): ReLUActivation()\n",
            "          (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "          (4): ReLUActivation()\n",
            "          (5): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (6): ReLUActivation()\n",
            "          (7): Linear(in_features=256, out_features=128, bias=True)\n",
            "          (8): ReLUActivation()\n",
            "          (9): Linear(in_features=128, out_features=68, bias=True)\n",
            "          (10): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoders): ModuleList(\n",
            "      (0): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=128, bias=True)\n",
            "          (1): ReLUActivation()\n",
            "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (3): ReLUActivation()\n",
            "          (4): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (5): ReLUActivation()\n",
            "          (6): Linear(in_features=512, out_features=1024, bias=True)\n",
            "          (7): ReLUActivation()\n",
            "          (8): Linear(in_features=1024, out_features=68, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (1): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=128, bias=True)\n",
            "          (1): ReLUActivation()\n",
            "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (3): ReLUActivation()\n",
            "          (4): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (5): ReLUActivation()\n",
            "          (6): Linear(in_features=512, out_features=1024, bias=True)\n",
            "          (7): ReLUActivation()\n",
            "          (8): Linear(in_features=1024, out_features=514, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (discriminators): ModuleList(\n",
            "      (0): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=64, bias=True)\n",
            "          (1): ReLUActivation()\n",
            "          (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "          (3): SigmoidActivation()\n",
            "        )\n",
            "      )\n",
            "      (1): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=514, out_features=64, bias=True)\n",
            "          (1): ReLUActivation()\n",
            "          (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "          (3): SigmoidActivation()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fusers): ModuleList(\n",
            "      (0): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (1): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (2): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (3): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (4): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (5): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (6): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (7): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (8): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "      (9): FuserManager(\n",
            "        (layer): WeightedMeanFuser()\n",
            "      )\n",
            "    )\n",
            "    (projectors): ModuleList(\n",
            "      (0): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (1): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (2): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (3): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (4): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (5): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (6): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (7): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (8): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "      (9): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=68, out_features=100, bias=True)\n",
            "          (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (2): ReLUActivation()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (clusters): ModuleList(\n",
            "      (0): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (1): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (2): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (3): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (4): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (5): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (6): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (7): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (8): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (9): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=100, out_features=5, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "TRAIN\n",
            "\n",
            "    (Epoch 1 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 1.607776403427124\n",
            "        discriminator: 0.5258381962776184\n",
            "        generator: 0.10556042194366455\n",
            "        reconstruction: 1.1319990158081055\n",
            "        translation: 1.144106149673462\n",
            "    Losses\n",
            "        contrastive: 0.8463783860206604\n",
            "        discriminator: 0.4821033179759979\n",
            "        generator: 0.04568704590201378\n",
            "        reconstruction: 0.8633363246917725\n",
            "        translation: 0.9672032594680786\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_1.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 9.936982154846191\n",
            "        reconstruction: 0.8633655905723572\n",
            "        self_entropy: -0.3374496102333069\n",
            "    Losses\n",
            "        ddc: 7.99841833114624\n",
            "        reconstruction: 0.8634004592895508\n",
            "        self_entropy: -0.34121403098106384\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_1.pt\n",
            "\n",
            "    (Epoch 2 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 1.6053650379180908\n",
            "        discriminator: 0.466343492269516\n",
            "        generator: 0.04085039719939232\n",
            "        reconstruction: 0.789702296257019\n",
            "        translation: 0.9319974780082703\n",
            "    Losses\n",
            "        contrastive: 1.1392134428024292\n",
            "        discriminator: 0.46487677097320557\n",
            "        generator: 0.030513867735862732\n",
            "        reconstruction: 0.7431091666221619\n",
            "        translation: 0.8752028346061707\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_2.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 6.349404811859131\n",
            "        reconstruction: 0.7432430386543274\n",
            "        self_entropy: -0.27671268582344055\n",
            "    Losses\n",
            "        ddc: 5.21491813659668\n",
            "        reconstruction: 0.7429499626159668\n",
            "        self_entropy: -0.2930223345756531\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_2.pt\n",
            "\n",
            "    (Epoch 3 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 1.01193106174469\n",
            "        discriminator: 0.4540179669857025\n",
            "        generator: 0.034407831728458405\n",
            "        reconstruction: 0.716720461845398\n",
            "        translation: 0.869586706161499\n",
            "    Losses\n",
            "        contrastive: 0.7089371085166931\n",
            "        discriminator: 0.45229145884513855\n",
            "        generator: 0.03329881653189659\n",
            "        reconstruction: 0.6676061749458313\n",
            "        translation: 0.8273772597312927\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_3.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 4.8121232986450195\n",
            "        reconstruction: 0.6676265597343445\n",
            "        self_entropy: -0.30756324529647827\n",
            "    Losses\n",
            "        ddc: 3.973172426223755\n",
            "        reconstruction: 0.6674097180366516\n",
            "        self_entropy: -0.32551324367523193\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_3.pt\n",
            "\n",
            "    (Epoch 4 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.6177937388420105\n",
            "        discriminator: 0.44810253381729126\n",
            "        generator: 0.03541345149278641\n",
            "        reconstruction: 0.6597833037376404\n",
            "        translation: 0.8231424689292908\n",
            "    Losses\n",
            "        contrastive: 0.4017811417579651\n",
            "        discriminator: 0.4496532380580902\n",
            "        generator: 0.03406403958797455\n",
            "        reconstruction: 0.6244139671325684\n",
            "        translation: 0.7673313021659851\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_4.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 3.928790807723999\n",
            "        reconstruction: 0.6240511536598206\n",
            "        self_entropy: -0.3465971052646637\n",
            "    Losses\n",
            "        ddc: 3.20515513420105\n",
            "        reconstruction: 0.6244747042655945\n",
            "        self_entropy: -0.3430011570453644\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_4.pt\n",
            "\n",
            "    (Epoch 5 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.4007014334201813\n",
            "        discriminator: 0.441620796918869\n",
            "        generator: 0.03932256996631622\n",
            "        reconstruction: 0.6156821846961975\n",
            "        translation: 0.7765294313430786\n",
            "    Losses\n",
            "        contrastive: 0.30590757727622986\n",
            "        discriminator: 0.4369492828845978\n",
            "        generator: 0.04264631122350693\n",
            "        reconstruction: 0.5766079425811768\n",
            "        translation: 0.7312462329864502\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_5.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 3.4317922592163086\n",
            "        reconstruction: 0.5770857930183411\n",
            "        self_entropy: -0.344878613948822\n",
            "    Losses\n",
            "        ddc: 3.000490188598633\n",
            "        reconstruction: 0.5769444704055786\n",
            "        self_entropy: -0.33148881793022156\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_5.pt\n",
            "\n",
            "    (Epoch 6 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.29418259859085083\n",
            "        discriminator: 0.43539363145828247\n",
            "        generator: 0.043809354305267334\n",
            "        reconstruction: 0.5716988444328308\n",
            "        translation: 0.7399924397468567\n",
            "    Losses\n",
            "        contrastive: 0.2384124994277954\n",
            "        discriminator: 0.432483047246933\n",
            "        generator: 0.04578346386551857\n",
            "        reconstruction: 0.5452931523323059\n",
            "        translation: 0.6957435607910156\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_6.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 3.1103897094726562\n",
            "        reconstruction: 0.5459414720535278\n",
            "        self_entropy: -0.3620085120201111\n",
            "    Losses\n",
            "        ddc: 2.7429895401000977\n",
            "        reconstruction: 0.5459890365600586\n",
            "        self_entropy: -0.36914581060409546\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_6.pt\n",
            "\n",
            "    (Epoch 7 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.23426273465156555\n",
            "        discriminator: 0.4263855218887329\n",
            "        generator: 0.05016497150063515\n",
            "        reconstruction: 0.542300283908844\n",
            "        translation: 0.7189224362373352\n",
            "    Losses\n",
            "        contrastive: 0.17227691411972046\n",
            "        discriminator: 0.4193001091480255\n",
            "        generator: 0.05572787672281265\n",
            "        reconstruction: 0.5069781541824341\n",
            "        translation: 0.6598751544952393\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_7.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.875652313232422\n",
            "        reconstruction: 0.5072551369667053\n",
            "        self_entropy: -0.34404560923576355\n",
            "    Losses\n",
            "        ddc: 2.6210410594940186\n",
            "        reconstruction: 0.5069811940193176\n",
            "        self_entropy: -0.34670591354370117\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_7.pt\n",
            "\n",
            "    (Epoch 8 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.1958426684141159\n",
            "        discriminator: 0.42711782455444336\n",
            "        generator: 0.05023328959941864\n",
            "        reconstruction: 0.5118108987808228\n",
            "        translation: 0.6842203736305237\n",
            "    Losses\n",
            "        contrastive: 0.137078195810318\n",
            "        discriminator: 0.4174431562423706\n",
            "        generator: 0.05593029409646988\n",
            "        reconstruction: 0.4732303023338318\n",
            "        translation: 0.6213940382003784\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_8.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.7380261421203613\n",
            "        reconstruction: 0.4733263850212097\n",
            "        self_entropy: -0.3572942316532135\n",
            "    Losses\n",
            "        ddc: 2.3352560997009277\n",
            "        reconstruction: 0.4730512201786041\n",
            "        self_entropy: -0.36985453963279724\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_8.pt\n",
            "\n",
            "    (Epoch 9 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.15195102989673615\n",
            "        discriminator: 0.41765135526657104\n",
            "        generator: 0.056659262627363205\n",
            "        reconstruction: 0.4848468005657196\n",
            "        translation: 0.6353673934936523\n",
            "    Losses\n",
            "        contrastive: 0.10428430140018463\n",
            "        discriminator: 0.42371439933776855\n",
            "        generator: 0.052064865827560425\n",
            "        reconstruction: 0.4627237915992737\n",
            "        translation: 0.5832757949829102\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_9.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.514873743057251\n",
            "        reconstruction: 0.4617975652217865\n",
            "        self_entropy: -0.3725254237651825\n",
            "    Losses\n",
            "        ddc: 2.2203354835510254\n",
            "        reconstruction: 0.4621390402317047\n",
            "        self_entropy: -0.3869653642177582\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_9.pt\n",
            "\n",
            "    (Epoch 10 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.10241445153951645\n",
            "        discriminator: 0.4137401282787323\n",
            "        generator: 0.05945103242993355\n",
            "        reconstruction: 0.4561519920825958\n",
            "        translation: 0.5986105799674988\n",
            "    Losses\n",
            "        contrastive: 0.08222735673189163\n",
            "        discriminator: 0.40918105840682983\n",
            "        generator: 0.06270470470190048\n",
            "        reconstruction: 0.4197395443916321\n",
            "        translation: 0.553335964679718\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_10.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.6146671772003174\n",
            "        reconstruction: 0.4193134307861328\n",
            "        self_entropy: -0.35979148745536804\n",
            "    Losses\n",
            "        ddc: 2.1737916469573975\n",
            "        reconstruction: 0.41966480016708374\n",
            "        self_entropy: -0.37044769525527954\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_10.pt\n",
            "\n",
            "    (Epoch 11 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.07762496173381805\n",
            "        discriminator: 0.40941527485847473\n",
            "        generator: 0.06242647394537926\n",
            "        reconstruction: 0.43284887075424194\n",
            "        translation: 0.5745978951454163\n",
            "    Losses\n",
            "        contrastive: 0.06535430997610092\n",
            "        discriminator: 0.402536004781723\n",
            "        generator: 0.06551007926464081\n",
            "        reconstruction: 0.39376261830329895\n",
            "        translation: 0.5107695460319519\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_11.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.3090879917144775\n",
            "        reconstruction: 0.3932102620601654\n",
            "        self_entropy: -0.36676108837127686\n",
            "    Losses\n",
            "        ddc: 2.0259149074554443\n",
            "        reconstruction: 0.39314213395118713\n",
            "        self_entropy: -0.3788264989852905\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_11.pt\n",
            "\n",
            "    (Epoch 12 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.06499990075826645\n",
            "        discriminator: 0.4091259241104126\n",
            "        generator: 0.06304565072059631\n",
            "        reconstruction: 0.40084514021873474\n",
            "        translation: 0.5249242782592773\n",
            "    Losses\n",
            "        contrastive: 0.05051249638199806\n",
            "        discriminator: 0.4030078053474426\n",
            "        generator: 0.06655095517635345\n",
            "        reconstruction: 0.36280447244644165\n",
            "        translation: 0.4803105890750885\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_12.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.213696002960205\n",
            "        reconstruction: 0.3625471293926239\n",
            "        self_entropy: -0.3782612085342407\n",
            "    Losses\n",
            "        ddc: 1.8506132364273071\n",
            "        reconstruction: 0.3622885048389435\n",
            "        self_entropy: -0.3909878730773926\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_12.pt\n",
            "\n",
            "    (Epoch 13 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.060837168246507645\n",
            "        discriminator: 0.4037146270275116\n",
            "        generator: 0.06694842875003815\n",
            "        reconstruction: 0.3797979950904846\n",
            "        translation: 0.49705231189727783\n",
            "    Losses\n",
            "        contrastive: 0.051131535321474075\n",
            "        discriminator: 0.4139488935470581\n",
            "        generator: 0.05920883268117905\n",
            "        reconstruction: 0.3507979214191437\n",
            "        translation: 0.44567418098449707\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_13.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.147002935409546\n",
            "        reconstruction: 0.3505500555038452\n",
            "        self_entropy: -0.3680027425289154\n",
            "    Losses\n",
            "        ddc: 1.8505302667617798\n",
            "        reconstruction: 0.3502243757247925\n",
            "        self_entropy: -0.37534099817276\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_13.pt\n",
            "\n",
            "    (Epoch 14 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.047818198800086975\n",
            "        discriminator: 0.4043819010257721\n",
            "        generator: 0.06603225320577621\n",
            "        reconstruction: 0.355643630027771\n",
            "        translation: 0.45493343472480774\n",
            "    Losses\n",
            "        contrastive: 0.03787649795413017\n",
            "        discriminator: 0.4054917097091675\n",
            "        generator: 0.06546350568532944\n",
            "        reconstruction: 0.3252525329589844\n",
            "        translation: 0.4078197777271271\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_14.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.8899977207183838\n",
            "        reconstruction: 0.3243102729320526\n",
            "        self_entropy: -0.38737526535987854\n",
            "    Losses\n",
            "        ddc: 1.6163077354431152\n",
            "        reconstruction: 0.3246201276779175\n",
            "        self_entropy: -0.3945922255516052\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_14.pt\n",
            "\n",
            "    (Epoch 15 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.04705589637160301\n",
            "        discriminator: 0.4020155668258667\n",
            "        generator: 0.06868557631969452\n",
            "        reconstruction: 0.3355374336242676\n",
            "        translation: 0.42776331305503845\n",
            "    Losses\n",
            "        contrastive: 0.036421939730644226\n",
            "        discriminator: 0.40711817145347595\n",
            "        generator: 0.06485418975353241\n",
            "        reconstruction: 0.3096807301044464\n",
            "        translation: 0.3771537244319916\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_15.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.8648185729980469\n",
            "        reconstruction: 0.31022191047668457\n",
            "        self_entropy: -0.3741576373577118\n",
            "    Losses\n",
            "        ddc: 1.6093405485153198\n",
            "        reconstruction: 0.3102431297302246\n",
            "        self_entropy: -0.372236430644989\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_15.pt\n",
            "\n",
            "    (Epoch 16 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.04147304594516754\n",
            "        discriminator: 0.4008192718029022\n",
            "        generator: 0.06917105615139008\n",
            "        reconstruction: 0.3171581029891968\n",
            "        translation: 0.4065037965774536\n",
            "    Losses\n",
            "        contrastive: 0.035447172820568085\n",
            "        discriminator: 0.39152637124061584\n",
            "        generator: 0.07550076395273209\n",
            "        reconstruction: 0.28522273898124695\n",
            "        translation: 0.3548336625099182\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_16.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.8398226499557495\n",
            "        reconstruction: 0.2857408821582794\n",
            "        self_entropy: -0.39741677045822144\n",
            "    Losses\n",
            "        ddc: 1.6105583906173706\n",
            "        reconstruction: 0.28548237681388855\n",
            "        self_entropy: -0.40275999903678894\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_16.pt\n",
            "\n",
            "    (Epoch 17 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.04142926633358002\n",
            "        discriminator: 0.39688920974731445\n",
            "        generator: 0.07174079120159149\n",
            "        reconstruction: 0.2985232174396515\n",
            "        translation: 0.37234705686569214\n",
            "    Losses\n",
            "        contrastive: 0.032852884382009506\n",
            "        discriminator: 0.40077105164527893\n",
            "        generator: 0.06868720799684525\n",
            "        reconstruction: 0.2681034803390503\n",
            "        translation: 0.33242592215538025\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_17.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.8383909463882446\n",
            "        reconstruction: 0.2684522271156311\n",
            "        self_entropy: -0.3904498815536499\n",
            "    Losses\n",
            "        ddc: 1.5728204250335693\n",
            "        reconstruction: 0.2690354883670807\n",
            "        self_entropy: -0.3912813663482666\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_17.pt\n",
            "\n",
            "    (Epoch 18 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.03521692007780075\n",
            "        discriminator: 0.39646440744400024\n",
            "        generator: 0.07194807380437851\n",
            "        reconstruction: 0.28195831179618835\n",
            "        translation: 0.35326460003852844\n",
            "    Losses\n",
            "        contrastive: 0.029516130685806274\n",
            "        discriminator: 0.39562204480171204\n",
            "        generator: 0.07257097959518433\n",
            "        reconstruction: 0.2579672634601593\n",
            "        translation: 0.31472036242485046\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_18.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 2.0380899906158447\n",
            "        reconstruction: 0.25984376668930054\n",
            "        self_entropy: -0.38710787892341614\n",
            "    Losses\n",
            "        ddc: 1.8061343431472778\n",
            "        reconstruction: 0.25869184732437134\n",
            "        self_entropy: -0.3978911340236664\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_18.pt\n",
            "\n",
            "    (Epoch 19 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.032118406146764755\n",
            "        discriminator: 0.3960201144218445\n",
            "        generator: 0.07248812168836594\n",
            "        reconstruction: 0.2688814401626587\n",
            "        translation: 0.3298284113407135\n",
            "    Losses\n",
            "        contrastive: 0.030631866306066513\n",
            "        discriminator: 0.3831372857093811\n",
            "        generator: 0.08857325464487076\n",
            "        reconstruction: 0.2479812353849411\n",
            "        translation: 0.2934268116950989\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_19.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.893951177597046\n",
            "        reconstruction: 0.24815165996551514\n",
            "        self_entropy: -0.4234373867511749\n",
            "    Losses\n",
            "        ddc: 1.6137866973876953\n",
            "        reconstruction: 0.24730391800403595\n",
            "        self_entropy: -0.4354238212108612\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_19.pt\n",
            "\n",
            "    (Epoch 20 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.03836127370595932\n",
            "        discriminator: 0.40086132287979126\n",
            "        generator: 0.06948729604482651\n",
            "        reconstruction: 0.25541794300079346\n",
            "        translation: 0.3145996928215027\n",
            "    Losses\n",
            "        contrastive: 0.02910679019987583\n",
            "        discriminator: 0.38995692133903503\n",
            "        generator: 0.07514435052871704\n",
            "        reconstruction: 0.2284935861825943\n",
            "        translation: 0.28208857774734497\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_20.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.973708152770996\n",
            "        reconstruction: 0.22876136004924774\n",
            "        self_entropy: -0.3728524148464203\n",
            "    Losses\n",
            "        ddc: 1.6868230104446411\n",
            "        reconstruction: 0.22816678881645203\n",
            "        self_entropy: -0.37453997135162354\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_20.pt\n",
            "\n",
            "    (Epoch 21 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.03410757705569267\n",
            "        discriminator: 0.3857440948486328\n",
            "        generator: 0.07961344718933105\n",
            "        reconstruction: 0.23701301217079163\n",
            "        translation: 0.2905467748641968\n",
            "    Losses\n",
            "        contrastive: 0.026994196698069572\n",
            "        discriminator: 0.39477893710136414\n",
            "        generator: 0.0732186958193779\n",
            "        reconstruction: 0.213931143283844\n",
            "        translation: 0.24627375602722168\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_21.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.79500412940979\n",
            "        reconstruction: 0.21440280973911285\n",
            "        self_entropy: -0.3972323536872864\n",
            "    Losses\n",
            "        ddc: 1.5151714086532593\n",
            "        reconstruction: 0.21516701579093933\n",
            "        self_entropy: -0.40633124113082886\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_21.pt\n",
            "\n",
            "    (Epoch 22 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.030234888195991516\n",
            "        discriminator: 0.3923751413822174\n",
            "        generator: 0.07534502446651459\n",
            "        reconstruction: 0.22014755010604858\n",
            "        translation: 0.2625875174999237\n",
            "    Losses\n",
            "        contrastive: 0.025609709322452545\n",
            "        discriminator: 0.3976266384124756\n",
            "        generator: 0.07060800492763519\n",
            "        reconstruction: 0.19803960621356964\n",
            "        translation: 0.22591644525527954\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_22.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.5974193811416626\n",
            "        reconstruction: 0.19878210127353668\n",
            "        self_entropy: -0.4031858444213867\n",
            "    Losses\n",
            "        ddc: 1.367110252380371\n",
            "        reconstruction: 0.1981354057788849\n",
            "        self_entropy: -0.41695842146873474\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_22.pt\n",
            "\n",
            "    (Epoch 23 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.023204242810606956\n",
            "        discriminator: 0.389240026473999\n",
            "        generator: 0.07749973237514496\n",
            "        reconstruction: 0.20310355722904205\n",
            "        translation: 0.23513711988925934\n",
            "    Losses\n",
            "        contrastive: 0.0215440783649683\n",
            "        discriminator: 0.39301079511642456\n",
            "        generator: 0.0738755539059639\n",
            "        reconstruction: 0.17714804410934448\n",
            "        translation: 0.20276786386966705\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_23.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.656703233718872\n",
            "        reconstruction: 0.17762193083763123\n",
            "        self_entropy: -0.4022604525089264\n",
            "    Losses\n",
            "        ddc: 1.4480464458465576\n",
            "        reconstruction: 0.17707441747188568\n",
            "        self_entropy: -0.4075108468532562\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_23.pt\n",
            "\n",
            "    (Epoch 24 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.023342324420809746\n",
            "        discriminator: 0.3864385783672333\n",
            "        generator: 0.0794883742928505\n",
            "        reconstruction: 0.19086122512817383\n",
            "        translation: 0.21970827877521515\n",
            "    Losses\n",
            "        contrastive: 0.020328985527157784\n",
            "        discriminator: 0.3914487361907959\n",
            "        generator: 0.07598605006933212\n",
            "        reconstruction: 0.1762038618326187\n",
            "        translation: 0.19431817531585693\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_24.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.9267504215240479\n",
            "        reconstruction: 0.17607757449150085\n",
            "        self_entropy: -0.41151607036590576\n",
            "    Losses\n",
            "        ddc: 1.6696926355361938\n",
            "        reconstruction: 0.17608027160167694\n",
            "        self_entropy: -0.42170244455337524\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_24.pt\n",
            "\n",
            "    (Epoch 25 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.023907333612442017\n",
            "        discriminator: 0.386454701423645\n",
            "        generator: 0.07960482686758041\n",
            "        reconstruction: 0.17817534506320953\n",
            "        translation: 0.2005390077829361\n",
            "    Losses\n",
            "        contrastive: 0.02100769802927971\n",
            "        discriminator: 0.39549800753593445\n",
            "        generator: 0.07242581993341446\n",
            "        reconstruction: 0.16093827784061432\n",
            "        translation: 0.17627814412117004\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_25.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.862442970275879\n",
            "        reconstruction: 0.1608314961194992\n",
            "        self_entropy: -0.37023261189460754\n",
            "    Losses\n",
            "        ddc: 1.642159104347229\n",
            "        reconstruction: 0.16087447106838226\n",
            "        self_entropy: -0.3840421736240387\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_25.pt\n",
            "\n",
            "    (Epoch 26 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.02200763113796711\n",
            "        discriminator: 0.3858529329299927\n",
            "        generator: 0.08001357316970825\n",
            "        reconstruction: 0.16479746997356415\n",
            "        translation: 0.18573397397994995\n",
            "    Losses\n",
            "        contrastive: 0.019429439678788185\n",
            "        discriminator: 0.3827100396156311\n",
            "        generator: 0.08153392374515533\n",
            "        reconstruction: 0.1487191617488861\n",
            "        translation: 0.16270706057548523\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_26.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.5374455451965332\n",
            "        reconstruction: 0.14854750037193298\n",
            "        self_entropy: -0.4233819246292114\n",
            "    Losses\n",
            "        ddc: 1.3078047037124634\n",
            "        reconstruction: 0.14873379468917847\n",
            "        self_entropy: -0.4362165033817291\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_26.pt\n",
            "\n",
            "    (Epoch 27 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.020708877593278885\n",
            "        discriminator: 0.38376277685165405\n",
            "        generator: 0.0814516469836235\n",
            "        reconstruction: 0.1573401838541031\n",
            "        translation: 0.17191234230995178\n",
            "    Losses\n",
            "        contrastive: 0.017745885998010635\n",
            "        discriminator: 0.3857935070991516\n",
            "        generator: 0.07946279644966125\n",
            "        reconstruction: 0.13967275619506836\n",
            "        translation: 0.14927902817726135\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_27.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.4380484819412231\n",
            "        reconstruction: 0.14017680287361145\n",
            "        self_entropy: -0.4439605176448822\n",
            "    Losses\n",
            "        ddc: 1.2274872064590454\n",
            "        reconstruction: 0.13951414823532104\n",
            "        self_entropy: -0.442931592464447\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_27.pt\n",
            "\n",
            "    (Epoch 28 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.022196270525455475\n",
            "        discriminator: 0.3819361627101898\n",
            "        generator: 0.08275134116411209\n",
            "        reconstruction: 0.1483628749847412\n",
            "        translation: 0.1608879268169403\n",
            "    Losses\n",
            "        contrastive: 0.020342163741588593\n",
            "        discriminator: 0.3799615204334259\n",
            "        generator: 0.08466700464487076\n",
            "        reconstruction: 0.1354808360338211\n",
            "        translation: 0.1413259208202362\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_28.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.6075623035430908\n",
            "        reconstruction: 0.13526488840579987\n",
            "        self_entropy: -0.4118465185165405\n",
            "    Losses\n",
            "        ddc: 1.4227529764175415\n",
            "        reconstruction: 0.13552165031433105\n",
            "        self_entropy: -0.41693535447120667\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_28.pt\n",
            "\n",
            "    (Epoch 29 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.018001127988100052\n",
            "        discriminator: 0.381365031003952\n",
            "        generator: 0.08332736790180206\n",
            "        reconstruction: 0.14356914162635803\n",
            "        translation: 0.15401384234428406\n",
            "    Losses\n",
            "        contrastive: 0.016436832025647163\n",
            "        discriminator: 0.3858034908771515\n",
            "        generator: 0.08004438877105713\n",
            "        reconstruction: 0.12715156376361847\n",
            "        translation: 0.13596253097057343\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_29.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.5694900751113892\n",
            "        reconstruction: 0.12694454193115234\n",
            "        self_entropy: -0.3916890621185303\n",
            "    Losses\n",
            "        ddc: 1.3599522113800049\n",
            "        reconstruction: 0.1277463734149933\n",
            "        self_entropy: -0.4001074433326721\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_29.pt\n",
            "\n",
            "    (Epoch 30 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.018768474459648132\n",
            "        discriminator: 0.37930041551589966\n",
            "        generator: 0.08494812250137329\n",
            "        reconstruction: 0.13092748820781708\n",
            "        translation: 0.14014172554016113\n",
            "    Losses\n",
            "        contrastive: 0.016959775239229202\n",
            "        discriminator: 0.37721318006515503\n",
            "        generator: 0.08720353245735168\n",
            "        reconstruction: 0.11722910404205322\n",
            "        translation: 0.12020771950483322\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_30.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.6658062934875488\n",
            "        reconstruction: 0.11629049479961395\n",
            "        self_entropy: -0.40338632464408875\n",
            "    Losses\n",
            "        ddc: 1.4391764402389526\n",
            "        reconstruction: 0.11678741127252579\n",
            "        self_entropy: -0.4121619760990143\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_30.pt\n",
            "\n",
            "    (Epoch 31 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.016825810074806213\n",
            "        discriminator: 0.3792565166950226\n",
            "        generator: 0.08498131483793259\n",
            "        reconstruction: 0.12161769717931747\n",
            "        translation: 0.12428165972232819\n",
            "    Losses\n",
            "        contrastive: 0.01490100659430027\n",
            "        discriminator: 0.3778454065322876\n",
            "        generator: 0.08583372086286545\n",
            "        reconstruction: 0.11108650267124176\n",
            "        translation: 0.10888919979333878\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_31.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.6288485527038574\n",
            "        reconstruction: 0.11130038648843765\n",
            "        self_entropy: -0.401775598526001\n",
            "    Losses\n",
            "        ddc: 1.4211738109588623\n",
            "        reconstruction: 0.11096959561109543\n",
            "        self_entropy: -0.41208338737487793\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_31.pt\n",
            "\n",
            "    (Epoch 32 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.0172102190554142\n",
            "        discriminator: 0.37775203585624695\n",
            "        generator: 0.08609896898269653\n",
            "        reconstruction: 0.11413180828094482\n",
            "        translation: 0.11731802672147751\n",
            "    Losses\n",
            "        contrastive: 0.014273188076913357\n",
            "        discriminator: 0.3774956166744232\n",
            "        generator: 0.08631696552038193\n",
            "        reconstruction: 0.10472360253334045\n",
            "        translation: 0.10485207289457321\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_32.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.6749595403671265\n",
            "        reconstruction: 0.10470753163099289\n",
            "        self_entropy: -0.405556857585907\n",
            "    Losses\n",
            "        ddc: 1.4512861967086792\n",
            "        reconstruction: 0.10486447811126709\n",
            "        self_entropy: -0.41047483682632446\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_32.pt\n",
            "\n",
            "    (Epoch 33 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.020553141832351685\n",
            "        discriminator: 0.3749863803386688\n",
            "        generator: 0.08803040534257889\n",
            "        reconstruction: 0.10640331357717514\n",
            "        translation: 0.11013331264257431\n",
            "    Losses\n",
            "        contrastive: 0.016767576336860657\n",
            "        discriminator: 0.38027140498161316\n",
            "        generator: 0.08325149118900299\n",
            "        reconstruction: 0.09579254686832428\n",
            "        translation: 0.09511150419712067\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_33.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.373160481452942\n",
            "        reconstruction: 0.09628814458847046\n",
            "        self_entropy: -0.46485263109207153\n",
            "    Losses\n",
            "        ddc: 1.1725164651870728\n",
            "        reconstruction: 0.09531570971012115\n",
            "        self_entropy: -0.4777892529964447\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_33.pt\n",
            "\n",
            "    (Epoch 34 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.015629494562745094\n",
            "        discriminator: 0.3732616603374481\n",
            "        generator: 0.08932452648878098\n",
            "        reconstruction: 0.09948958456516266\n",
            "        translation: 0.1016477644443512\n",
            "    Losses\n",
            "        contrastive: 0.013951924629509449\n",
            "        discriminator: 0.3775324523448944\n",
            "        generator: 0.08562873303890228\n",
            "        reconstruction: 0.09469375014305115\n",
            "        translation: 0.09254453331232071\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_34.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.5907917022705078\n",
            "        reconstruction: 0.09495686739683151\n",
            "        self_entropy: -0.3862878084182739\n",
            "    Losses\n",
            "        ddc: 1.3675308227539062\n",
            "        reconstruction: 0.09484482556581497\n",
            "        self_entropy: -0.39339300990104675\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_34.pt\n",
            "\n",
            "    (Epoch 35 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.014295472763478756\n",
            "        discriminator: 0.3720414638519287\n",
            "        generator: 0.09023425728082657\n",
            "        reconstruction: 0.09732265770435333\n",
            "        translation: 0.09794227033853531\n",
            "    Losses\n",
            "        contrastive: 0.013936497271060944\n",
            "        discriminator: 0.37682288885116577\n",
            "        generator: 0.08637472242116928\n",
            "        reconstruction: 0.08700990676879883\n",
            "        translation: 0.0857507660984993\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_35.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.5686652660369873\n",
            "        reconstruction: 0.08628398180007935\n",
            "        self_entropy: -0.4348255693912506\n",
            "    Losses\n",
            "        ddc: 1.346928358078003\n",
            "        reconstruction: 0.08699362725019455\n",
            "        self_entropy: -0.4354093074798584\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_35.pt\n",
            "\n",
            "    (Epoch 36 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.013806459493935108\n",
            "        discriminator: 0.3708871603012085\n",
            "        generator: 0.09072300791740417\n",
            "        reconstruction: 0.0905318558216095\n",
            "        translation: 0.09006845951080322\n",
            "    Losses\n",
            "        contrastive: 0.011703407391905785\n",
            "        discriminator: 0.3772191107273102\n",
            "        generator: 0.08546211570501328\n",
            "        reconstruction: 0.08372244983911514\n",
            "        translation: 0.0795239731669426\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_36.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.448879599571228\n",
            "        reconstruction: 0.08372735977172852\n",
            "        self_entropy: -0.43169844150543213\n",
            "    Losses\n",
            "        ddc: 1.2460401058197021\n",
            "        reconstruction: 0.08390714228153229\n",
            "        self_entropy: -0.4428091049194336\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_36.pt\n",
            "\n",
            "    (Epoch 37 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.013776537030935287\n",
            "        discriminator: 0.369214802980423\n",
            "        generator: 0.0919724777340889\n",
            "        reconstruction: 0.08711763471364975\n",
            "        translation: 0.08434360474348068\n",
            "    Losses\n",
            "        contrastive: 0.01116863638162613\n",
            "        discriminator: 0.37195825576782227\n",
            "        generator: 0.0898938849568367\n",
            "        reconstruction: 0.07924401015043259\n",
            "        translation: 0.07296997308731079\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_37.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.456511378288269\n",
            "        reconstruction: 0.07871317863464355\n",
            "        self_entropy: -0.43064120411872864\n",
            "    Losses\n",
            "        ddc: 1.2865816354751587\n",
            "        reconstruction: 0.07883383333683014\n",
            "        self_entropy: -0.4398035407066345\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_37.pt\n",
            "\n",
            "    (Epoch 38 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.015459966845810413\n",
            "        discriminator: 0.36807626485824585\n",
            "        generator: 0.09289997816085815\n",
            "        reconstruction: 0.08127065002918243\n",
            "        translation: 0.07864467054605484\n",
            "    Losses\n",
            "        contrastive: 0.012454074807465076\n",
            "        discriminator: 0.37127479910850525\n",
            "        generator: 0.09012427181005478\n",
            "        reconstruction: 0.07438500970602036\n",
            "        translation: 0.06738056242465973\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_38.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.2952454090118408\n",
            "        reconstruction: 0.07345541566610336\n",
            "        self_entropy: -0.441570520401001\n",
            "    Losses\n",
            "        ddc: 1.1303616762161255\n",
            "        reconstruction: 0.07402286678552628\n",
            "        self_entropy: -0.45394670963287354\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_38.pt\n",
            "\n",
            "    (Epoch 39 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.01313288975507021\n",
            "        discriminator: 0.3668164312839508\n",
            "        generator: 0.09374260157346725\n",
            "        reconstruction: 0.07611637562513351\n",
            "        translation: 0.07168214023113251\n",
            "    Losses\n",
            "        contrastive: 0.012015964835882187\n",
            "        discriminator: 0.3679664433002472\n",
            "        generator: 0.09286584705114365\n",
            "        reconstruction: 0.07064153999090195\n",
            "        translation: 0.06395804136991501\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_39.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.4990822076797485\n",
            "        reconstruction: 0.07046665251255035\n",
            "        self_entropy: -0.41913747787475586\n",
            "    Losses\n",
            "        ddc: 1.341017484664917\n",
            "        reconstruction: 0.07001117616891861\n",
            "        self_entropy: -0.42868712544441223\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_39.pt\n",
            "\n",
            "    (Epoch 40 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.012053458020091057\n",
            "        discriminator: 0.3651221990585327\n",
            "        generator: 0.09500642865896225\n",
            "        reconstruction: 0.06924917548894882\n",
            "        translation: 0.06496966630220413\n",
            "    Losses\n",
            "        contrastive: 0.011139382608234882\n",
            "        discriminator: 0.37025758624076843\n",
            "        generator: 0.09050413966178894\n",
            "        reconstruction: 0.06286341696977615\n",
            "        translation: 0.05652732402086258\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_40.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.4477252960205078\n",
            "        reconstruction: 0.06278935074806213\n",
            "        self_entropy: -0.4698813855648041\n",
            "    Losses\n",
            "        ddc: 1.2626124620437622\n",
            "        reconstruction: 0.06303556263446808\n",
            "        self_entropy: -0.4773619771003723\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_40.pt\n",
            "\n",
            "    (Epoch 41 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.012202573008835316\n",
            "        discriminator: 0.36573320627212524\n",
            "        generator: 0.09449434280395508\n",
            "        reconstruction: 0.06568291038274765\n",
            "        translation: 0.061313893646001816\n",
            "    Losses\n",
            "        contrastive: 0.010289994068443775\n",
            "        discriminator: 0.3650922477245331\n",
            "        generator: 0.09550748020410538\n",
            "        reconstruction: 0.05996062979102135\n",
            "        translation: 0.05320759862661362\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_41.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.3024390935897827\n",
            "        reconstruction: 0.059662286192178726\n",
            "        self_entropy: -0.4666329622268677\n",
            "    Losses\n",
            "        ddc: 1.1028821468353271\n",
            "        reconstruction: 0.059549037367105484\n",
            "        self_entropy: -0.47021883726119995\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_41.pt\n",
            "\n",
            "    (Epoch 42 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.01221865601837635\n",
            "        discriminator: 0.3653346002101898\n",
            "        generator: 0.09466008841991425\n",
            "        reconstruction: 0.061922285705804825\n",
            "        translation: 0.05782124027609825\n",
            "    Losses\n",
            "        contrastive: 0.010161269456148148\n",
            "        discriminator: 0.3626045286655426\n",
            "        generator: 0.09815199673175812\n",
            "        reconstruction: 0.05696054920554161\n",
            "        translation: 0.05144714564085007\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_42.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.3261343240737915\n",
            "        reconstruction: 0.05686328560113907\n",
            "        self_entropy: -0.43474072217941284\n",
            "    Losses\n",
            "        ddc: 1.1710596084594727\n",
            "        reconstruction: 0.05711984634399414\n",
            "        self_entropy: -0.45125505328178406\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_42.pt\n",
            "\n",
            "    (Epoch 43 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.010571775957942009\n",
            "        discriminator: 0.36565694212913513\n",
            "        generator: 0.09436037391424179\n",
            "        reconstruction: 0.058896660804748535\n",
            "        translation: 0.05425688251852989\n",
            "    Losses\n",
            "        contrastive: 0.010034275241196156\n",
            "        discriminator: 0.3662642538547516\n",
            "        generator: 0.09387847036123276\n",
            "        reconstruction: 0.05811985209584236\n",
            "        translation: 0.05098944902420044\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_43.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.5460666418075562\n",
            "        reconstruction: 0.05843281000852585\n",
            "        self_entropy: -0.4155435860157013\n",
            "    Losses\n",
            "        ddc: 1.3225282430648804\n",
            "        reconstruction: 0.05896320566534996\n",
            "        self_entropy: -0.4283228814601898\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_43.pt\n",
            "\n",
            "    (Epoch 44 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.011621737852692604\n",
            "        discriminator: 0.3638186454772949\n",
            "        generator: 0.09595030546188354\n",
            "        reconstruction: 0.05750424414873123\n",
            "        translation: 0.05373171344399452\n",
            "    Losses\n",
            "        contrastive: 0.010428253561258316\n",
            "        discriminator: 0.3666441738605499\n",
            "        generator: 0.0929270014166832\n",
            "        reconstruction: 0.05325610935688019\n",
            "        translation: 0.04736272618174553\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_44.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.3886761665344238\n",
            "        reconstruction: 0.053047940135002136\n",
            "        self_entropy: -0.4345194399356842\n",
            "    Losses\n",
            "        ddc: 1.1605629920959473\n",
            "        reconstruction: 0.0533134788274765\n",
            "        self_entropy: -0.43831920623779297\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_44.pt\n",
            "\n",
            "    (Epoch 45 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.010093605145812035\n",
            "        discriminator: 0.3627382814884186\n",
            "        generator: 0.09628260135650635\n",
            "        reconstruction: 0.05541139096021652\n",
            "        translation: 0.051832880824804306\n",
            "    Losses\n",
            "        contrastive: 0.008720815181732178\n",
            "        discriminator: 0.367477148771286\n",
            "        generator: 0.09242101013660431\n",
            "        reconstruction: 0.05291800573468208\n",
            "        translation: 0.04533875361084938\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_45.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.3630136251449585\n",
            "        reconstruction: 0.052379220724105835\n",
            "        self_entropy: -0.4532841444015503\n",
            "    Losses\n",
            "        ddc: 1.155258059501648\n",
            "        reconstruction: 0.052643924951553345\n",
            "        self_entropy: -0.46646758913993835\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_45.pt\n",
            "\n",
            "    (Epoch 46 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.009962932206690311\n",
            "        discriminator: 0.36187317967414856\n",
            "        generator: 0.09711681306362152\n",
            "        reconstruction: 0.05206897482275963\n",
            "        translation: 0.04773395508527756\n",
            "    Losses\n",
            "        contrastive: 0.008699551224708557\n",
            "        discriminator: 0.36667099595069885\n",
            "        generator: 0.09294331818819046\n",
            "        reconstruction: 0.04772517830133438\n",
            "        translation: 0.04243532568216324\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_46.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.3545280694961548\n",
            "        reconstruction: 0.047709908336400986\n",
            "        self_entropy: -0.4691679775714874\n",
            "    Losses\n",
            "        ddc: 1.1568137407302856\n",
            "        reconstruction: 0.04772162437438965\n",
            "        self_entropy: -0.48479312658309937\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_46.pt\n",
            "\n",
            "    (Epoch 47 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.010916680097579956\n",
            "        discriminator: 0.36104777455329895\n",
            "        generator: 0.09754317998886108\n",
            "        reconstruction: 0.04982652887701988\n",
            "        translation: 0.045405272394418716\n",
            "    Losses\n",
            "        contrastive: 0.008883540518581867\n",
            "        discriminator: 0.3624386489391327\n",
            "        generator: 0.09660268574953079\n",
            "        reconstruction: 0.04826897382736206\n",
            "        translation: 0.04164410009980202\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_47.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.4707622528076172\n",
            "        reconstruction: 0.04848514497280121\n",
            "        self_entropy: -0.4061828851699829\n",
            "    Losses\n",
            "        ddc: 1.255391001701355\n",
            "        reconstruction: 0.04835299029946327\n",
            "        self_entropy: -0.3998219966888428\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_47.pt\n",
            "\n",
            "    (Epoch 48 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.010683347471058369\n",
            "        discriminator: 0.36121609807014465\n",
            "        generator: 0.09737192094326019\n",
            "        reconstruction: 0.04968554899096489\n",
            "        translation: 0.04590881988406181\n",
            "    Losses\n",
            "        contrastive: 0.009091798216104507\n",
            "        discriminator: 0.3640856444835663\n",
            "        generator: 0.09524793177843094\n",
            "        reconstruction: 0.04946316033601761\n",
            "        translation: 0.04278113692998886\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_48.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.242290735244751\n",
            "        reconstruction: 0.049354806542396545\n",
            "        self_entropy: -0.45746999979019165\n",
            "    Losses\n",
            "        ddc: 1.0861384868621826\n",
            "        reconstruction: 0.04941605404019356\n",
            "        self_entropy: -0.4631354808807373\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_48.pt\n",
            "\n",
            "    (Epoch 49 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.009934618137776852\n",
            "        discriminator: 0.36078569293022156\n",
            "        generator: 0.09761416167020798\n",
            "        reconstruction: 0.04979805648326874\n",
            "        translation: 0.04494420439004898\n",
            "    Losses\n",
            "        contrastive: 0.009142977185547352\n",
            "        discriminator: 0.3620492219924927\n",
            "        generator: 0.09615188837051392\n",
            "        reconstruction: 0.04592398926615715\n",
            "        translation: 0.03907060995697975\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_49.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.46989107131958\n",
            "        reconstruction: 0.04616998881101608\n",
            "        self_entropy: -0.4539240002632141\n",
            "    Losses\n",
            "        ddc: 1.2552214860916138\n",
            "        reconstruction: 0.046268023550510406\n",
            "        self_entropy: -0.45480814576148987\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_49.pt\n",
            "\n",
            "    (Epoch 50 / 50)\n",
            "    ========== Schedule 0: translation ==========\n",
            "    Losses\n",
            "        contrastive: 0.0099383145570755\n",
            "        discriminator: 0.35850170254707336\n",
            "        generator: 0.09915974736213684\n",
            "        reconstruction: 0.04600172117352486\n",
            "        translation: 0.041116006672382355\n",
            "    Losses\n",
            "        contrastive: 0.008378894999623299\n",
            "        discriminator: 0.3592595160007477\n",
            "        generator: 0.09907688945531845\n",
            "        reconstruction: 0.04338361695408821\n",
            "        translation: 0.03665520250797272\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/best.pt\n",
            "            Saving model to saved_patchseq/single_layer/models/train_0_translation/epoch_50.pt\n",
            "    ========== Schedule 1: clustering ==========\n",
            "    Losses\n",
            "        ddc: 1.4377694129943848\n",
            "        reconstruction: 0.04335146024823189\n",
            "        self_entropy: -0.44662079215049744\n",
            "    Losses\n",
            "        ddc: 1.2865917682647705\n",
            "        reconstruction: 0.04344021528959274\n",
            "        self_entropy: -0.46272894740104675\n",
            "            Saving model to saved_patchseq/single_layer/models/train_1_clustering/epoch_50.pt\n",
            "Total: 9.08 mins\n"
          ]
        }
      ],
      "source": [
        "from src import UnitedNet\n",
        "import anndata as ad\n",
        "import os, shutil\n",
        "import time \n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model = UnitedNet(\n",
        "    device=device,\n",
        "    log_path=log_path,\n",
        "    model_path=model_path,\n",
        "    tensorboard_path=tensorboard_path,\n",
        "    verbose=verbose,\n",
        ")\n",
        "\n",
        "# ======================================== Register Data ========================================\n",
        "checkpoint_time = time.time()\n",
        "model.register_anndatas(\n",
        "    adatas_train, \n",
        "    label_index=0, label_key=label_key,\n",
        "    technique=data_technique,\n",
        ")\n",
        "print_time('Register data', checkpoint_time)\n",
        "print(model.model)\n",
        "\n",
        "# ======================================== Train ========================================\n",
        "checkpoint_time = time.time()\n",
        "clean_results(log_path, model_path, tensorboard_path)\n",
        "model.train(\n",
        "    'unsupervised_group_identification', n_epoch=train_epoch, learning_rate=learning_rate, batch_size=train_batch,\n",
        "    save_best_model=True, checkpoint=checkpoint,\n",
        ")\n",
        "\n",
        "print_time(f'Total', start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 0.0\n",
            "2 0.6663029288478295\n",
            "3 0.7093068470052806\n",
            "4 0.492362100506377\n",
            "5 0.4833087267638936\n",
            "6 0.3557739055343197\n",
            "7 0.36423060955610614\n",
            "8 0.38321729325808573\n",
            "9 0.41302156063368023\n",
            "10 0.43316706376327285\n",
            "11 0.48745742668747877\n",
            "12 0.43592010869515335\n",
            "13 0.37779099215061\n",
            "14 0.49028137608660355\n",
            "15 0.46142612598932486\n",
            "16 0.38171760914847475\n",
            "17 0.4702770447382624\n",
            "18 0.3736523273438997\n",
            "19 0.39713074479016963\n",
            "20 0.45103669318249706\n",
            "21 0.4854492894289575\n",
            "22 0.43677492724580036\n",
            "23 0.42911738250301495\n",
            "24 0.3588399861550133\n",
            "25 0.4878582670483996\n",
            "26 0.44925177255628107\n",
            "27 0.4844368807405012\n",
            "28 0.3591363151511391\n",
            "29 0.35584063029907426\n",
            "30 0.46812692876601186\n",
            "31 0.4511836554359859\n",
            "32 0.4865115922545949\n",
            "33 0.4709573383188222\n",
            "34 0.47443175911487706\n",
            "35 0.4432946335152109\n",
            "36 0.48352176309177847\n",
            "37 0.46478361106638283\n",
            "38 0.45042557615087614\n",
            "39 0.4613856210913737\n",
            "40 0.3858097982963604\n",
            "41 0.3559791358864154\n",
            "42 0.44769471265370503\n",
            "43 0.42830903084030386\n",
            "44 0.4402653239908076\n",
            "45 0.44490046011790196\n",
            "46 0.4401924607815514\n",
            "47 0.4622260153633578\n",
            "48 0.44796721709144005\n",
            "49 0.46392026828439514\n",
            "50 0.46392026828439514\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, train_epoch+1):\n",
        "    model.load_model(f'saved_patchseq/single_layer/models/train_0_translation/epoch_{epoch}.pt')\n",
        "    model.set_verbose(False)\n",
        "    print(epoch, model.evaluate()['ari'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModelConfig(input_sizes=[68, 514], output_size=5, class_weights=[2.0318181818181817, 1.0642857142857143, 4.966666666666667, 0.39210526315789473, 1.2246575342465753], encoders=[MLPConfig(input_size=68, output_size=68, hidden_sizes=[1024, 512, 256, 128], is_binary_input=False, activations=[ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), None], dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=[False, False, False, False, True]), MLPConfig(input_size=514, output_size=68, hidden_sizes=[1024, 512, 256, 128], is_binary_input=False, activations=[ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), None], dropouts=[0.1, 0.0, 0.0, 0.0, 0.0], use_biases=True, use_batch_norms=False, use_layer_norms=[False, False, False, False, True])], decoders=[MLPConfig(input_size=68, output_size=68, hidden_sizes=[128, 256, 512, 1024], is_binary_input=False, activations=[ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), None], dropouts=0.0, use_biases=True, use_batch_norms=[False, False, False, False, False], use_layer_norms=[False, False, False, False, False]), MLPConfig(input_size=68, output_size=514, hidden_sizes=[128, 256, 512, 1024], is_binary_input=False, activations=[ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), ActivationConfig(method='relu'), None], dropouts=0.0, use_biases=True, use_batch_norms=[False, False, False, False, False], use_layer_norms=[False, False, False, False, False])], discriminators=[MLPConfig(input_size=68, output_size=1, hidden_sizes=[64], is_binary_input=False, activations=[ActivationConfig(method='relu'), ActivationConfig(method='sigmoid')], dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=514, output_size=1, hidden_sizes=[64], is_binary_input=False, activations=[ActivationConfig(method='relu'), ActivationConfig(method='sigmoid')], dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False)], fusers=[FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean'), FuserConfig(n_modality=2, method='weighted_mean')], projectors=[MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True), MLPConfig(input_size=68, output_size=100, hidden_sizes=[], is_binary_input=False, activations=ActivationConfig(method='relu'), dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=True)], clusters=[MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False), MLPConfig(input_size=100, output_size=5, hidden_sizes=[], is_binary_input=False, activations=None, dropouts=0.0, use_biases=True, use_batch_norms=False, use_layer_norms=False)])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FRLdFX-T2MUJ"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "from src import UnitedNet\n",
        "def evaluate_adatas(path, adatas):\n",
        "    model = UnitedNet(\n",
        "        device=device,\n",
        "        log_path=None,\n",
        "        model_path=None,\n",
        "        tensorboard_path=None,\n",
        "        verbose=False,\n",
        "    )\n",
        "    model.load_model(path)\n",
        "\n",
        "    return model.evaluate(\n",
        "        adatas,\n",
        "        label_index_evaluate=0, label_key_evaluate=label_key,\n",
        "        batch_index_evaluate=0, batch_key_evaluate=use_batch_key,\n",
        "    )['ari']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzqUEUYK2J0v",
        "outputId": "2d3ff430-f898-4567-b4b1-ab056f4eeda0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_batch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31680\\1365718045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0madatas_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madatas_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0madatas_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_adatas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madatas_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madatas_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_batch' is not defined"
          ]
        }
      ],
      "source": [
        "final_model_path = f'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\n",
        "aris = []\n",
        "for epoch in range(1, train_epoch+1):\n",
        "    adatas_train, adatas_test = split_data(test_batch)\n",
        "    adatas_all = concat_adatas(adatas_train, adatas_test)\n",
        "\n",
        "    model_path = f'{final_model_path}/epoch_{epoch}.pt'\n",
        "    aris.append(evaluate_adatas(model_path, adatas_test))\n",
        "\n",
        "for epoch in range(1, train_epoch+1):\n",
        "    print('epoch', epoch, aris[epoch-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGATLbJKMATU"
      },
      "source": [
        "#### Infer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_6pdX11hQ1"
      },
      "source": [
        "##### Infer Commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLL1F7b6MATU"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "from src import UnitedNet\n",
        "def infer_adatas(path, adatas, eval_only=True):\n",
        "    model = UnitedNet(\n",
        "        device=device,\n",
        "        log_path=None,\n",
        "        model_path=None,\n",
        "        tensorboard_path=None,\n",
        "        verbose=True,\n",
        "    )\n",
        "    model.load_model(path)\n",
        "\n",
        "    model.evaluate(\n",
        "        adatas,\n",
        "        label_index_evaluate=0, label_key_evaluate=label_key,\n",
        "        batch_index_evaluate=0, batch_key_evaluate=use_batch_key,\n",
        "    )\n",
        "\n",
        "    if not eval_only:\n",
        "      adata_inferred = model.infer(\n",
        "          adatas,\n",
        "          modalities_provided=list(range(len(adatas))),\n",
        "          batch_index_infer=0, batch_key_infer=use_batch_key, \n",
        "          modality_sizes=[adata.shape[1] for adata in adatas]\n",
        "      )\n",
        "      \n",
        "      adata_inferred.obs[modified_batch_key] = list(adatas[0].obs[modified_batch_key])\n",
        "      sc.pl.umap(adata_inferred, color=[modified_batch_key])\n",
        "\n",
        "      adata_inferred.obs['batch'] = list(adatas[0].obs['batch'])\n",
        "      sc.pl.umap(adata_inferred, color=['batch'])\n",
        "      \n",
        "      sc.pl.umap(adata_inferred, color=['predicted_label'])\n",
        "      \n",
        "      adata_inferred.obs[label_key] = list(adatas[0].obs[label_key])\n",
        "      sc.pl.umap(adata_inferred, color=[label_key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4596WEwMATW"
      },
      "source": [
        "##### Infer on train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9xkneiQrOQpV",
        "outputId": "77e6ab8a-4a1b-44d3-d1e0-2b3a264f7ad6"
      },
      "outputs": [],
      "source": [
        "final_model_path = f'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\n",
        "for epoch in range(1, train_epoch+1):\n",
        "    print('='*20, 'epoch', epoch)\n",
        "\n",
        "    adatas_train, adatas_test = split_data(test_batch)\n",
        "    adatas_all = concat_adatas(adatas_train, adatas_test)\n",
        "\n",
        "    model_path = f'{final_model_path}/epoch_{epoch}.pt'\n",
        "    infer_adatas(model_path, adatas_test)\n",
        "    infer_adatas(model_path, adatas_all, eval_only=False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P1xrExR9t-0m",
        "outputId": "8d4f74a2-55d3-403b-98b4-772e2b3053ad"
      },
      "outputs": [],
      "source": [
        "final_model_path = f'{root_path}/models/{test_batch}/transfer_{final_step}_classification'\n",
        "for epoch in [train_epoch]:\n",
        "    print('='*20, 'epoch', epoch)\n",
        "\n",
        "    adatas_train, adatas_test = split_data(test_batch)\n",
        "    adatas_all = concat_adatas(adatas_train, adatas_test)\n",
        "\n",
        "    model_path = f'{final_model_path}/epoch_{epoch}.pt'\n",
        "    infer_adatas(model_path, adatas_test)\n",
        "    infer_adatas(model_path, adatas_all, eval_only=False)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xts0jTbV1hQs",
        "elk-mSRoB2bv",
        "I83RcMOb1hQv",
        "OK9SInRzMATN",
        "3m_6pdX11hQ1"
      ],
      "machine_shape": "hm",
      "name": "train_then_transfer C0_T1_C3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8dd4d9bf6ad6a171f666a3d65bcfced95842184592f01e2d9451b920ba48bd03"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('MMCGAN_ET')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
