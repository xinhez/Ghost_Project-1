from email.policy import default
from src.configs.config import ActivationConfig, ModelConfig, MLPConfig, FuserConfig

atacseq_config = ModelConfig(
    input_sizes=[13634, 4000],
    output_size=22,
    class_weights=[
        1.6961354217126863,
        0.2887293617489143,
        1.5671623234533518,
        0.6108257414227564,
        0.7431317061667256,
        0.2636941022894631,
        2.830803090190442,
        0.6955019973956609,
        2.621018048739915,
        2.7786350410016754,
        69.87250554323725,
        3.9082847575344166,
        1.6588145496657367,
        3.4936252771618626,
        0.45199948363406867,
        0.613967579784125,
        2.0565489786595315,
        8.952414772727273,
        1.2817775066097215,
        1.0505217188385505,
        3.480890312603557,
        2.606708578046158,
    ],
    encoders=[
        MLPConfig(
            input_size=13634,
            output_size=64,
            hidden_sizes=[64, 64],
            is_binary_input=True,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=4000,
            output_size=64,
            hidden_sizes=[64, 64],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
    ],
    decoders=[
        MLPConfig(
            input_size=64,
            output_size=13634,
            hidden_sizes=[64, 64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=[False, False, False],
            use_layer_norms=[False, False, False],
        ),
        MLPConfig(
            input_size=64,
            output_size=4000,
            hidden_sizes=[64, 64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=[False, False, False],
            use_layer_norms=[False, False, False],
        ),
    ],
    discriminators=[
        MLPConfig(
            input_size=13634,
            output_size=1,
            hidden_sizes=[64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=4000,
            output_size=1,
            hidden_sizes=[64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
    ],
    fusers=[FuserConfig(n_modality=2, method="weighted_mean")],
    projectors=[
        MLPConfig(
            input_size=64,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        )
    ],
    clusters=[
        MLPConfig(
            input_size=100,
            output_size=22,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        )
    ],
)

dlpfc_config = ModelConfig(
    input_sizes=[2365, 50, 100],
    output_size=7,
    class_weights=[
        1.3669890597748533,
        2.412176151306586,
        0.3761474294090957,
        1.937786568371842,
        0.9295525606469003,
        1.077565304336958,
        1.480357142857143,
    ],
    encoders=[
        MLPConfig(
            input_size=2365,
            output_size=50,
            hidden_sizes=[512, 256, 128],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=True,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=50,
            output_size=50,
            hidden_sizes=[256, 128, 64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=True,
            use_layer_norms=False,
        ),
    ],
    decoders=[
        MLPConfig(
            input_size=50,
            output_size=2365,
            hidden_sizes=[128, 256, 512],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=[True, True, True, False],
            use_layer_norms=[False, False, False, False],
        ),
        MLPConfig(
            input_size=50,
            output_size=50,
            hidden_sizes=[64, 128, 256],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=[True, True, True, False],
            use_layer_norms=[False, False, False, False],
        ),
    ],
    discriminators=[
        MLPConfig(
            input_size=2365,
            output_size=1,
            hidden_sizes=[64, 32],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=50,
            output_size=1,
            hidden_sizes=[64, 32],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=1,
            hidden_sizes=[64, 32],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
    ],
    fusers=[FuserConfig(n_modality=3, method="weighted_mean")],
    projectors=[
        MLPConfig(
            input_size=50,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        )
    ],
    clusters=[
        MLPConfig(
            input_size=100,
            output_size=7,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=False,
            use_batch_norms=False,
            use_layer_norms=False,
        )
    ],
)

patchseq_config = ModelConfig(
    input_sizes=[68, 514],
    output_size=5,
    class_weights=[],
    encoders=[
        MLPConfig(
            input_size=68,
            output_size=68,
            hidden_sizes=[1024, 512, 256, 128],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=[False, False, False, False, True],
        ),
        MLPConfig(
            input_size=514,
            output_size=68,
            hidden_sizes=[1024, 512, 256, 128],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=[0.1, 0.0, 0.0, 0.0, 0.0],
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=[False, False, False, False, True],
        ),
    ],
    decoders=[
        MLPConfig(
            input_size=68,
            output_size=68,
            hidden_sizes=[128, 256, 512, 1024],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=[False, False, False, False, False],
            use_layer_norms=[False, False, False, False, False],
        ),
        MLPConfig(
            input_size=68,
            output_size=514,
            hidden_sizes=[128, 256, 512, 1024],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                ActivationConfig(method="relu"),
                None,
            ],
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=[False, False, False, False, False],
            use_layer_norms=[False, False, False, False, False],
        ),
    ],
    discriminators=[
        MLPConfig(
            input_size=68,
            output_size=1,
            hidden_sizes=[64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=514,
            output_size=1,
            hidden_sizes=[64],
            is_binary_input=False,
            activations=[
                ActivationConfig(method="relu"),
                ActivationConfig(method="sigmoid"),
            ],
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
    ],
    fusers=[
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
        FuserConfig(n_modality=2, method="weighted_mean"),
    ],
    projectors=[
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
        MLPConfig(
            input_size=68,
            output_size=100,
            hidden_sizes=[],
            is_binary_input=False,
            activations=ActivationConfig(method="relu"),
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=True,
        ),
    ],
    clusters=[
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
        MLPConfig(
            input_size=100,
            output_size=5,
            hidden_sizes=[],
            is_binary_input=False,
            activations=None,
            dropouts=0.0,
            use_biases=True,
            use_batch_norms=False,
            use_layer_norms=False,
        ),
    ],
)

default_configs = {
    "dlpfc": dlpfc_config,
    "patchseq": patchseq_config,
}
